# SEO Ads Expert v1.9 Implementation Plan - REFINED

**Technical SEO Intelligence & Site Health System**

> **Status**: Refined with GPT-5 feedback (2025-09-19)
> **Timeline**: 2-3 weeks
> **Priority**: CLI consolidation is critical path (Day 0.5)

## 1. Executive Summary

v1.9 transforms SEO Ads Expert from keyword-focused to comprehensive site intelligence. The system will crawl, analyze, and optimize entire websites through automated discovery, link graph analysis, and intelligent sitemap generation.

### Key Differentiators
- **Internal HTML Crawler**: Full control vs external tool dependencies
- **SQLite Link Graph**: Efficient storage with relationship analysis
- **Sectioned XML Sitemaps**: Intelligent partitioning for large sites
- **GSC Indexation Audit**: Performance-based optimization recommendations

## 2. Architecture Overview

### 2.1 Core Systems

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI Router    â”‚â”€â”€â”€â–¶â”‚  Crawl Engine   â”‚â”€â”€â”€â–¶â”‚  Link Analyzer  â”‚
â”‚  (consolidated) â”‚    â”‚   (internal)    â”‚    â”‚  (graph-based)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sitemap Gen    â”‚    â”‚  Index Monitor  â”‚    â”‚  Health Report  â”‚
â”‚  (sectioned)    â”‚    â”‚   (GSC API)     â”‚    â”‚  (actionable)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Technology Stack
- **HTTP Client**: undici (performance, HTTP/2)
- **HTML Parser**: cheerio (jQuery-like API)
- **Concurrency**: p-queue (controlled parallelism)
- **XML Generation**: fast-xml-parser
- **Database**: SQLite with better-sqlite3

## 3. Implementation Phases

### Phase 1: CLI Consolidation (Day 0.5) - CRITICAL PATH

**Problem**: 47+ commands scattered across 6 files with execSync anti-patterns

**Solution**: Unified subcommand structure
```typescript
// src/cli/index.ts - New main entry point
import { Command } from 'commander';

const program = new Command();
program
  .name('seo-ads')
  .description('SEO & Google Ads Expert Tool v1.9');

// Subcommands
program.addCommand(createPlanCommand());      // from cli.ts
program.addCommand(createExperimentCommand()); // from cli-experiments.ts
program.addCommand(createAlertsCommand());    // from cli-alerts.ts
program.addCommand(createMicrosoftCommand()); // from cli-microsoft.ts
program.addCommand(createCrawlCommand());     // NEW v1.9
program.addCommand(createSitemapCommand());   // NEW v1.9
program.addCommand(createHealthCommand());    // NEW v1.9

program.parse();
```

**Benefits**:
- Single entry point: `npx tsx src/cli/index.ts <subcommand>`
- Consistent error handling and logging
- Shared middleware (auth, rate limiting, validation)

### Phase 2: Internal HTML Crawler (Days 1-3)

**Decision Rationale**: Build internal vs Screaming Frog
- âœ… Complete control over discovery logic
- âœ… Custom filtering and depth controls
- âœ… Real-time link graph construction
- âœ… Integration with existing auth/rate limiting
- âŒ More initial development time

**Core Implementation**:
```typescript
// src/crawl/crawler.ts
export class SiteAnalyzer {
  constructor(
    private config: CrawlConfig,
    private db: Database,
    private linkGraph: LinkGraphAnalyzer
  ) {}

  async crawlSite(startUrl: string): Promise<CrawlResults> {
    const discovered = new Set<string>([startUrl]);
    const queue = new PQueue({ concurrency: this.config.maxConcurrency });

    for (const url of discovered) {
      queue.add(() => this.crawlPage(url, discovered));
    }

    await queue.onIdle();
    return this.generateResults();
  }

  private async crawlPage(url: string, discovered: Set<string>): Promise<void> {
    const response = await this.fetch(url);
    const $ = cheerio.load(response.body);

    // Extract page data
    const pageData = this.extractPageData($, url);
    this.db.insertPage(pageData);

    // Discover links
    const links = this.extractLinks($, url);
    links.forEach(link => {
      discovered.add(link.href);
      this.linkGraph.addEdge(url, link.href, link.context);
    });
  }
}
```

**Crawl Database Schema**:
```sql
CREATE TABLE IF NOT EXISTS crawl_pages (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  url TEXT NOT NULL UNIQUE,
  canonical_url TEXT,
  status INTEGER,
  title TEXT,
  meta_description TEXT,
  h1 TEXT,
  word_count INTEGER,
  noindex INTEGER NOT NULL DEFAULT 0,
  nofollow INTEGER NOT NULL DEFAULT 0,
  robots_allowed INTEGER NOT NULL DEFAULT 1,
  depth INTEGER,
  section TEXT, -- For sitemap organization
  last_crawled DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS crawl_links (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  from_url TEXT NOT NULL,
  to_url TEXT NOT NULL,
  anchor_text TEXT,
  link_type TEXT, -- 'internal', 'external', 'mailto', etc.
  context TEXT,   -- surrounding content
  crawl_session_id TEXT,
  FOREIGN KEY (from_url) REFERENCES crawl_pages(url),
  FOREIGN KEY (to_url) REFERENCES crawl_pages(url)
);

CREATE INDEX idx_crawl_links_from ON crawl_links(from_url);
CREATE INDEX idx_crawl_links_to ON crawl_links(to_url);
```

### Phase 3: Link Graph Analysis (Days 4-5)

**SQLite vs Neo4j Decision**: SQLite sufficient for site-scale analysis
- âœ… Existing infrastructure integration
- âœ… Recursive CTE support for graph queries
- âœ… Performance adequate for <100k pages
- âœ… Simpler deployment and maintenance

**Core Analysis Queries**:
```sql
-- Find orphan pages (no incoming internal links)
WITH internal_targets AS (
  SELECT DISTINCT to_url
  FROM crawl_links
  WHERE link_type = 'internal'
)
SELECT p.url, p.title
FROM crawl_pages p
LEFT JOIN internal_targets it ON p.url = it.to_url
WHERE it.to_url IS NULL
  AND p.status = 200
  AND p.noindex = 0;

-- Calculate PageRank approximation
WITH RECURSIVE page_rank AS (
  SELECT url, 1.0 as rank, 0 as iteration
  FROM crawl_pages
  UNION ALL
  SELECT
    p.url,
    0.15 + 0.85 * COALESCE(
      (SELECT SUM(pr2.rank / link_counts.out_degree)
       FROM crawl_links cl
       JOIN page_rank pr2 ON cl.from_url = pr2.url
       JOIN (SELECT from_url, COUNT(*) as out_degree
             FROM crawl_links GROUP BY from_url) link_counts
         ON cl.from_url = link_counts.from_url
       WHERE cl.to_url = p.url), 0.15
    ) as rank,
    pr.iteration + 1
  FROM crawl_pages p
  JOIN page_rank pr ON p.url = pr.url
  WHERE pr.iteration < 10
)
SELECT url, rank FROM page_rank WHERE iteration = 10;
```

### Phase 4: Intelligent Sitemap Generation (Days 6-7)

**Sectioned Approach**: Organize by site structure and importance
```typescript
// src/sitemap/sitemap-generator.ts
export class SitemapGenerator {
  async generateSitemaps(pages: CrawlPage[]): Promise<SitemapFile[]> {
    const sections = this.organizeBySections(pages);
    const sitemaps: SitemapFile[] = [];

    for (const [section, sectionPages] of sections) {
      const chunks = this.chunkPages(sectionPages, 50000); // URL limit

      for (let i = 0; i < chunks.length; i++) {
        const sitemap = await this.generateSectionSitemap(
          section,
          chunks[i],
          i + 1
        );
        sitemaps.push(sitemap);
      }
    }

    // Generate index sitemap
    const indexSitemap = this.generateIndexSitemap(sitemaps);
    return [indexSitemap, ...sitemaps];
  }

  private organizeBySections(pages: CrawlPage[]): Map<string, CrawlPage[]> {
    const sections = new Map<string, CrawlPage[]>();

    for (const page of pages) {
      const section = this.detectSection(page.url);
      if (!sections.has(section)) {
        sections.set(section, []);
      }
      sections.get(section)!.push(page);
    }

    return sections;
  }

  private detectSection(url: string): string {
    const path = new URL(url).pathname;

    // Common section patterns
    if (path.includes('/blog/')) return 'blog';
    if (path.includes('/products/')) return 'products';
    if (path.includes('/docs/')) return 'documentation';
    if (path.match(/^\/[^\/]+\/$/) && path !== '/') return 'categories';

    return 'main';
  }
}
```

### Phase 5: GSC Integration & Health Monitoring (Days 8-10)

**Indexation Analysis**: Compare crawl results with GSC data
```typescript
// src/health/indexation-analyzer.ts
export class IndexationAnalyzer {
  async analyzeIndexationHealth(domain: string): Promise<HealthReport> {
    const [crawlPages, gscPages] = await Promise.all([
      this.getCrawlablePages(),
      this.getGSCIndexedPages(domain)
    ]);

    const analysis = {
      discoveredNotIndexed: this.findDiscoveredNotIndexed(crawlPages, gscPages),
      indexedNotCrawlable: this.findIndexedNotCrawlable(crawlPages, gscPages),
      duplicateIndexation: this.findDuplicateContent(gscPages),
      lowQualityIndexed: this.findLowQualityPages(crawlPages, gscPages)
    };

    return this.generateActionableReport(analysis);
  }

  private async getGSCIndexedPages(domain: string): Promise<GSCPage[]> {
    const query = {
      startDate: this.getDateDaysAgo(30),
      endDate: this.getDateDaysAgo(1),
      dimensions: ['page'],
      rowLimit: 25000
    };

    return this.gscClient.searchanalytics.query({
      siteUrl: `sc-domain:${domain}`,
      requestBody: query
    });
  }
}
```

## 4. New CLI Commands

### 4.1 Crawl Commands
```bash
# Start comprehensive site crawl
npx tsx src/cli/index.ts crawl start --url https://littlebearapps.com

# View crawl results
npx tsx src/cli/index.ts crawl results --session latest

# Find technical issues
npx tsx src/cli/index.ts crawl analyze --type orphans
npx tsx src/cli/index.ts crawl analyze --type duplicates
npx tsx src/cli/index.ts crawl analyze --type broken-links
```

### 4.2 Sitemap Commands
```bash
# Generate sitemaps from crawl data
npx tsx src/cli/index.ts sitemap generate --output ./sitemaps/

# Validate existing sitemaps
npx tsx src/cli/index.ts sitemap validate --url https://littlebearapps.com/sitemap.xml

# Submit sitemaps to GSC
npx tsx src/cli/index.ts sitemap submit --domain littlebearapps.com
```

### 4.3 Health Commands
```bash
# Comprehensive site health check
npx tsx src/cli/index.ts health check --domain littlebearapps.com

# Indexation analysis
npx tsx src/cli/index.ts health indexation --domain littlebearapps.com

# Performance recommendations
npx tsx src/cli/index.ts health recommendations --domain littlebearapps.com
```

## 5. Integration Strategy

### 5.1 Database Schema Extension
```sql
-- Extend existing database with v1.9 tables
ALTER TABLE plans ADD COLUMN crawl_session_id TEXT;
ALTER TABLE plans ADD COLUMN sitemap_generated INTEGER DEFAULT 0;

-- Link crawl data to existing keyword/competitor analysis
CREATE TABLE IF NOT EXISTS crawl_keyword_mapping (
  url TEXT,
  keyword TEXT,
  relevance_score REAL,
  target_match INTEGER DEFAULT 0,
  FOREIGN KEY (url) REFERENCES crawl_pages(url)
);
```

### 5.2 Existing System Enhancement
- **Keywords â†’ Page Mapping**: Connect keyword targets to actual pages
- **Competitor Analysis**: Crawl competitor sites for gap analysis
- **Landing Page Optimization**: Use crawl data to validate ad group mappings

## 6. Performance & Scalability

### 6.1 Crawl Performance
- **Concurrency**: 5-10 concurrent requests (respectful crawling)
- **Rate Limiting**: 1-2 RPS per domain (avoid triggering rate limits)
- **Memory Management**: Stream processing for large sites
- **Resume Capability**: Checkpoint system for interrupted crawls

### 6.2 Database Optimization
```sql
-- Performance indexes for common queries
CREATE INDEX idx_pages_status_noindex ON crawl_pages(status, noindex);
CREATE INDEX idx_pages_section ON crawl_pages(section);
CREATE INDEX idx_links_type ON crawl_links(link_type);

-- Cleanup old crawl data
DELETE FROM crawl_pages WHERE last_crawled < date('now', '-30 days');
```

## 6.3 Cost Management
- **GSC API Quota**: 1,000 requests/day (plan batch operations)
- **Storage**: SQLite file size monitoring and archiving
- **Processing**: Background job queue for large site analysis

## 6.4 Error Handling & Recovery
```typescript
// Robust error handling patterns
export class CrawlErrorHandler {
  async handlePageError(url: string, error: Error): Promise<void> {
    if (error instanceof TimeoutError) {
      // Retry with longer timeout
      return this.retryWithBackoff(url, { timeout: 30000 });
    }

    if (error instanceof HTTPError && error.status === 429) {
      // Rate limited - exponential backoff
      await this.waitAndRetry(url, error.headers['retry-after']);
      return;
    }

    // Log error and continue
    this.logger.warn(`Failed to crawl ${url}: ${error.message}`);
    this.db.insertPageError(url, error);
  }
}
```

## 6.5 v1.8 Compatibility Assessment

**Integration Points with v1.8 Systems**:

âœ… **Compatible**:
- **Entity Coverage System**: Crawl data enhances entity extraction from actual page content
- **Schema Generation**: Page crawl provides real content for schema validation
- **Content Planning**: Site structure analysis improves content gap detection
- **Link Optimization**: Internal link graph directly supports link suggestion engine

âœ… **Enhanced**:
- **CLI Integration**: v1.9 commands extend existing CLI structure
- **Database Schema**: New tables complement existing v1.8 schema
- **Orchestration**: V18Integration class can incorporate crawl results

âœ… **Backward Compatible**:
- All existing v1.8 commands continue to work unchanged
- Database migrations handle new tables gracefully
- Existing workflows unaffected by new crawl functionality

## 7. Testing Strategy

### 7.1 Unit Tests
- Crawler component testing with mock HTTP responses
- Link graph analysis algorithm validation
- Sitemap generation with various site structures
- CLI command argument parsing and validation

### 7.2 Integration Tests
- End-to-end crawl of test site (littlebearapps.com subset)
- GSC API integration with sandbox data
- Database transaction consistency
- Error recovery scenarios

### 7.3 Performance Tests
- Large site crawling (1000+ pages)
- Concurrent crawl session handling
- Memory usage profiling
- Database query performance under load

## 8. Success Metrics

### 8.1 Technical Metrics
- **Crawl Speed**: 50-100 pages/minute
- **Discovery Rate**: 95%+ of site pages found
- **Link Accuracy**: 99%+ correct link extraction
- **Sitemap Compliance**: W3C XML sitemap validation

### 8.2 Business Metrics
- **Issue Detection**: Automated finding of 10+ SEO issues per site
- **Time Savings**: Reduce manual site audit from 4 hours to 30 minutes
- **Actionability**: 80%+ of recommendations implementable
- **GSC Integration**: Real-time indexation status monitoring

## 9. Risk Mitigation

### 9.1 Technical Risks
- **Rate Limiting**: Implement respectful crawling with backoff
- **Memory Usage**: Stream processing and pagination
- **Site Blocking**: User-agent rotation and request spacing
- **Data Quality**: Robust HTML parsing and fallback strategies

### 9.2 Business Risks
- **API Quotas**: GSC API usage monitoring and optimization
- **Performance Impact**: Background processing and incremental updates
- **Complexity**: Phased rollout with feature flags
- **Maintenance**: Comprehensive error logging and monitoring

## 10. Output Artifacts & Data Contracts

### 10.1 File Structure
```
plans/<product_or_site>/<date>/
â”œâ”€â”€ link_graph.json
â”œâ”€â”€ internal_link_opportunities.csv
â”œâ”€â”€ sitemaps/
â”‚   â”œâ”€â”€ core.xml
â”‚   â”œâ”€â”€ use-cases.xml
â”‚   â”œâ”€â”€ blog.xml
â”‚   â”œâ”€â”€ images.xml
â”‚   â””â”€â”€ sitemap_index.xml
â”œâ”€â”€ robots_audit.md
â”œâ”€â”€ indexation_gaps.json
â”œâ”€â”€ bing_webmaster.md
â””â”€â”€ indexnow_ping.log
```

### 10.2 Data Contracts

**link_graph.json**:
```json
{
  "schemaVersion": "1.0",
  "generatedAt": "2025-09-19T12:34:56Z",
  "site": "https://littlebearapps.com",
  "nodes": [
    {
      "url": "/convertmyfile/webp-to-png",
      "absUrl": "https://littlebearapps.com/convertmyfile/webp-to-png",
      "type": "use-case|product|blog|docs|main|category",
      "status": 200,
      "in": 5,
      "out": 8,
      "title": "WebP to PNG converter",
      "canonical": "/convertmyfile/webp-to-png",
      "noindex": false,
      "depth": 2,
      "lastmod": "2025-09-18"
    }
  ],
  "edges": [
    {
      "src": "/convertmyfile/",
      "dst": "/convertmyfile/webp-to-png",
      "anchor": "WebPâ†’PNG converter",
      "rel": "contextual|nav|footer|breadcrumb",
      "nofollow": false,
      "contextSnippet": "â€¦convert WebP to PNG in secondsâ€¦"
    }
  ],
  "summary": {
    "pages": 124,
    "orphans": 0,
    "avg_out": 7.2,
    "avg_in": 4.8,
    "broken_links": 3
  }
}
```

**internal_link_opportunities.csv**:
| source_url | anchor_text | target_url | rationale | strength | rel | section_hint | evidence_in_links | evidence_semantic_score | guardrails_ok |
|------------|-------------|------------|-----------|----------|-----|--------------|-------------------|-------------------------|---------------|
| /blog/image-formats | WebP to PNG guide | /convertmyfile/webp-to-png | use-case needs 3+ inbound from distinct sources; high semantic fit | 3 | contextual | blog | 1 | 0.84 | true |

**indexation_gaps.json**:
```json
{
  "schemaVersion": "1.0",
  "generatedAt": "2025-09-19T12:34:56Z",
  "site": "https://littlebearapps.com",
  "summary": {
    "discovered_not_indexed": 3,
    "crawled_not_indexed": 2,
    "excluded_duplicates": 1
  },
  "items": [
    {
      "url": "/palettekit/palette-generator",
      "state": "Discovered - currently not indexed",
      "evidence": {
        "gsc": {"lastCrawled": null, "impressions": 0},
        "crawl": {"in_links": 1, "noindex": false, "status": 200}
      },
      "fix": "add internal links; include in sitemap; ensure unique H1/meta",
      "severity": "medium"
    }
  ]
}
```

## 11. CLI Command Aliases (Backward Compatibility)

### 11.1 Command Mapping
- `seo-ads link-graph` â†’ `seo-ads crawl start --url <site> + auto-emit link_graph.json`
- `seo-ads sitemap-emit` â†’ `seo-ads sitemap generate [--sections] [--pretty]`
- `seo-ads robots-audit` â†’ `seo-ads health robots-audit --site <site>`
- `seo-ads indexation-audit` â†’ `seo-ads health indexation --site <site> [--gsc path]`
- `seo-ads indexnow` â†’ `seo-ads bing indexnow --urls <file> [--dry-run]`

### 11.2 Implementation
```typescript
// src/cli/index.ts - Add backward compatibility aliases
program
  .command('link-graph')
  .description('Build link graph (alias to crawl start + emit)')
  .requiredOption('--site <url>')
  .option('--max-depth <n>', 'Depth limit', '4')
  .option('--budget <n>', 'Page budget', '500')
  .action(async (opts) => {
    await handleCrawlStart({
      url: opts.site,
      maxDepth: +opts.maxDepth,
      budget: +opts.budget,
      emitLinkGraph: true
    });
  });

program
  .command('robots-audit')
  .description('Run robots.txt audit (alias)')
  .requiredOption('--site <url>')
  .action((opts) => handleRobotsAudit(opts));

program
  .command('indexnow')
  .description('Send IndexNow pings (alias)')
  .requiredOption('--urls <path>')
  .option('--engine <name>', 'bing|yandex', 'bing')
  .option('--dry-run', 'Do not send; log only', false)
  .action((opts) => handleIndexNow(opts));
```

## 12. Robots.txt Audit Implementation

### 12.1 Module Design
```typescript
// src/robots/robots-auditor.ts
export interface RobotsAuditResult {
  sitemaps: string[];
  findings: Finding[];
  severity: 'HIGH' | 'MEDIUM' | 'LOW';
}

export async function auditRobots(site: string, options: { crawlDb?: Database }) {
  const robotsUrl = new URL('/robots.txt', site).toString();
  const res = await request(robotsUrl, { method: 'GET' });
  const body = await res.body.text();

  const parser = robotsParser(robotsUrl, body);
  const findings: Finding[] = [];

  // Sitemap checks
  const sitemaps = body.match(/^sitemap:\s*(.+)$/gim)?.map(l => l.split(':')[1].trim()) ?? [];
  if (!sitemaps.length) {
    findings.push({
      severity: 'MEDIUM',
      message: 'No Sitemap directive found',
      fix: 'Add Sitemap: <url> lines'
    });
  }

  // Overbroad disallow detection
  if (options.crawlDb) {
    const samples = options.crawlDb
      .prepare("SELECT url FROM crawl_pages WHERE status=200 AND noindex=0 LIMIT 500")
      .all();

    for (const row of samples) {
      if (!parser.isAllowed(new URL(row.url, site).toString(), '*')) {
        findings.push({
          severity: 'HIGH',
          message: `Crawlable 200 page blocked by robots: ${row.url}`,
          fix: 'Relax Disallow rule or add explicit Allow'
        });
        break;
      }
    }
  }

  return { sitemaps, findings };
}
```

### 12.2 Output Format (robots_audit.md)
```markdown
# Robots.txt Audit Report

## Summary
- **HIGH**: 1 (Blocked essential assets)
- **MEDIUM**: 1 (Missing Sitemap directive)
- **LOW**: 2 (Overbroad Disallow patterns)

## Sitemap Directives
- âœ… Found: https://littlebearapps.com/sitemap_index.xml
- âŒ Missing: Individual section sitemaps

## Essential Assets
- ğŸš¨ **HIGH**: JavaScript blocked: /static/main.js
- ğŸš¨ **HIGH**: CSS blocked: /static/styles.css

## Recommended Actions
1. Add explicit Allow rules for essential assets
2. Include all section sitemaps in robots.txt
3. Review Disallow patterns for unintended scope
```

## 13. IndexNow Integration

### 13.1 Implementation
- **Daily Quota**: 10,000 URLs per domain (Bing 2025 limit)
- **Endpoint**: https://www.bing.com/indexnow
- **Key Validation**: Verify hosted at https://domain.com/<KEY>

```typescript
// src/bing/indexnow.ts
export async function sendIndexNow(
  domain: string,
  urls: string[],
  opts: {
    key: string;
    keyLocation?: string;
    engine?: 'bing'|'yandex';
    dryRun?: boolean
  }
) {
  const endpoint = opts.engine === 'yandex'
    ? 'https://yandex.com/indexnow'
    : 'https://www.bing.com/indexnow';

  const payload = {
    host: domain,
    key: opts.key,
    keyLocation: opts.keyLocation ?? `https://${domain}/${opts.key}`,
    urlList: urls
  };

  if (opts.dryRun) {
    return logPing(domain, opts.engine ?? 'bing', urls.length, 0, 'dry-run');
  }

  const res = await fetch(endpoint, {
    method: 'POST',
    headers: {'content-type': 'application/json'},
    body: JSON.stringify(payload)
  });

  await logPing(domain, opts.engine ?? 'bing', urls.length, res.status, null);

  if (res.status !== 202) {
    throw new Error(`IndexNow failed: ${res.status}`);
  }
}
```

### 13.2 Quota Tracking
```sql
CREATE TABLE IF NOT EXISTS indexnow_quota (
  day TEXT,
  domain TEXT,
  sent INT DEFAULT 0,
  PRIMARY KEY (day, domain)
);
```

### 13.3 Log Format (indexnow_ping.log)
```json
{"ts":"2025-09-19T12:34:56Z","domain":"littlebearapps.com","engine":"bing","urls":250,"status":202,"retryAfter":null,"error":null}
```

## 14. Database Compatibility Views

### 14.1 Schema Extensions
```sql
-- Add content_hash for fact table compatibility
ALTER TABLE crawl_pages ADD COLUMN content_hash TEXT;
CREATE INDEX IF NOT EXISTS idx_crawl_last ON crawl_pages(last_crawled);

-- GSC indexation tracking
CREATE TABLE IF NOT EXISTS gsc_indexation (
  date TEXT,
  url TEXT,
  gsc_state TEXT,
  PRIMARY KEY (date, url)
);
```

### 14.2 Compatibility Views
```sql
-- fact_crawl view for ideas document compatibility
CREATE VIEW IF NOT EXISTS fact_crawl AS
SELECT
  datetime(last_crawled) AS captured_at,
  url,
  status,
  content_hash,
  COALESCE((
    SELECT COUNT(*) FROM crawl_links cl
    WHERE cl.from_url = crawl_pages.url AND cl.link_type='internal'
  ), 0) AS out_links,
  COALESCE((
    SELECT COUNT(*) FROM crawl_links cl
    WHERE cl.to_url = crawl_pages.url AND cl.link_type='internal'
  ), 0) AS in_links,
  noindex AS noindex,
  canonical_url AS canonical
FROM crawl_pages;

-- fact_indexation view
CREATE VIEW IF NOT EXISTS fact_indexation AS
SELECT date, url, gsc_state FROM gsc_indexation;
```

## 15. Acceptance Criteria

Based on v1.9 ideas document requirements:

### 15.1 Functional Acceptance
- âœ… **Zero orphan use-case pages** after applying suggested links
- âœ… **Sitemaps validate** and are accepted in Google Search Console
- âœ… **indexation_gaps.json** contains actionable per-URL fixes (not generic advice)
- âœ… **Robots audit flags** any sitemap omissions or accidental disallows

### 15.2 Technical Acceptance
- âœ… **Mini-site fixtures** (10-15 pages) for testing
- âœ… **Golden snapshots** for orphan detection, link opportunities, sitemap XML
- âœ… **Real GSC CSV** import and join logic validation
- âœ… **Robots tests** for blocked /extensions/ with high-severity flagging

### 15.3 Performance Acceptance
- âœ… **Crawl constraints**: Depth â‰¤4, budget â‰¤500, concurrency 2-3, timeout 6-8s
- âœ… **Respectful crawling**: robots.txt compliance, rate limiting
- âœ… **Quota management**: IndexNow 10K/day per domain enforcement

## 16. Future Expansion (v2.0 Preview)

v1.9 establishes the foundation for v2.0's advanced optimization features:
- **Crawl Data â†’ Budget Optimization**: Site performance informs ad spend allocation
- **Link Graph â†’ Bid Strategies**: Page authority influences keyword bidding
- **Content Analysis â†’ Creative Generation**: Page content drives ad copy optimization
- **Technical SEO â†’ Landing Page Score**: Site health affects Quality Score

---

**Timeline**: 2-3 weeks
**Dependencies**: Existing v1.8 infrastructure
**Approval**: Ready for implementation
**GPT-5 Review**: Completed 2025-09-19 âœ…
**Completeness Check**: âœ… All gaps from v1.9 ideas document addressed (2025-09-19)
**Added Sections**: Artifacts, CLI aliases, Robots audit, IndexNow, Database views, Acceptance criteria
