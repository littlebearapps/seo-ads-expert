# v1.4 Implementation Plan - Close the Loop (Performance â†’ Action)

## Executive Summary

**Goal**: Transform static keyword plans into an adaptive system that learns from real Google Ads performance data, automatically proposing optimizations to reduce waste and improve Quality Score.

**Timeline**: 3-5 days
**Priority**: HIGH - Direct ROI through waste reduction and QS improvement
**Complexity**: Medium-High (new ingestion pipelines, n-gram analysis)

## Core Deliverables

### 1. Waste Analysis & Negative Keywords
- **waste_report.md**: Detailed analysis of high-spend/no-conversion search terms
- **negatives_proposed.csv**: Google Ads Editor import-ready negative keywords
- **negatives_proposed.json**: API-ready format for automated application

### 2. Quality Score Optimization
- **qs_triage.md**: Ad groups ranked by QS issues with specific fixes
- **Automated recommendations** for Expected CTR, Ad Relevance, Landing Page Experience

### 3. Paid/Organic Synergy
- **paid_organic_gaps.md**: Opportunities where SEO wins but ads don't cover (and vice versa)
- **Cross-channel recommendations** for keyword and page alignment

## Technical Architecture

### Phase 1: Data Ingestion Layer (Day 1)

#### Task 1.1: Google Ads Search Terms Connector
```typescript
// src/connectors/google-ads-performance.ts
export class GoogleAdsPerformanceConnector {
  // GAQL approach (primary)
  async fetchSearchTerms(customerId: string, dateRange: DateRange): Promise<SearchTermData[]>
  
  // CSV fallback approach
  async importSearchTermsCSV(filePath: string): Promise<SearchTermData[]>
  
  // Quality Score fetching
  async fetchQualityScores(customerId: string): Promise<QualityScoreData[]>
}
```

**Implementation Notes**:
- Use GAQL for real-time data: `SELECT search_term, clicks, impressions, cost, conversions FROM search_term_view`
- CSV import path: `inputs/google_ads/search_terms_<product>_<period>.csv`
- Cache with 24h TTL to reduce API calls

#### Task 1.2: Analytics Integration
```typescript
// src/connectors/analytics-connector.ts
export class AnalyticsConnector {
  // GA4 events (CWS_Click, Waitlist_Submit)
  async fetchConversionEvents(dateRange: DateRange): Promise<ConversionEvent[]>
  
  // Plausible pageviews by UTM (optional)
  async fetchPageviews(utmParams: UTMParams): Promise<PageviewData[]>
}
```

### Phase 2: N-gram Analysis Engine (Day 2)

#### Task 2.1: N-gram Processor
```typescript
// src/analyzers/ngram-engine.ts
export class NgramEngine {
  private stopWords = ['the', 'a', 'an', 'and', 'or', 'but', ...];
  private minLength = 2;
  
  // Core n-gram extraction
  extractNgrams(searchTerms: string[], n: number): Map<string, NgramStats>
  
  // Waste identification
  identifyWastedSpend(ngrams: Map<string, NgramStats>): WastedTerm[]
  
  // Negative keyword generation
  generateNegatives(wastedTerms: WastedTerm[]): NegativeKeyword[]
}
```

**Key Logic**:
- Lowercase normalization
- Remove stop words
- Aggregate 30-day spend & conversions
- Flag zero-conversion terms with spend > $10
- Flag low-CTR terms (<0.5%) with >100 impressions

#### Task 2.2: Negative Placement Rules
```typescript
// src/analyzers/negative-placement.ts
export class NegativePlacementEngine {
  // Placement hierarchy
  determineLevel(negative: NegativeKeyword): 'shared' | 'campaign' | 'adGroup'
  
  // Shared list: non-intent words (jobs, tutorial, free)
  // Campaign level: platform mismatches (firefox, safari)
  // Ad group level: tight use-case cleanup
}
```

### Phase 3: Quality Score Triage System (Day 3)

#### Task 3.1: QS Analyzer
```typescript
// src/analyzers/quality-score-analyzer.ts
export class QualityScoreAnalyzer {
  // Join QS data with landing page health
  async analyzeQualityScore(qsData: QualityScoreData[], urlHealth: URLHealthData[]): Promise<QSAnalysis[]>
  
  // Bucket issues by component
  categorizeIssues(analysis: QSAnalysis): {
    adRelevance: AdRelevanceIssue[]
    lpExperience: LPExperienceIssue[]
    expectedCTR: ExpectedCTRIssue[]
  }
  
  // Generate specific fixes
  generateRecommendations(issues: CategorizedIssues): QSRecommendation[]
}
```

**Recommendation Logic**:
- **Ad Relevance < 5**: Add primary keyword to headline/description
- **LP Experience < 5**: Fix noindex/404/slow pages, add internal links
- **Expected CTR < 5**: Add proof/urgency headline, test extensions

#### Task 3.2: Paid/Organic Gap Analysis
```typescript
// src/analyzers/paid-organic-analyzer.ts
export class PaidOrganicAnalyzer {
  // Identify gaps
  findGaps(paidData: PaidData[], organicData: OrganicData[]): {
    seoWinsNoPaid: Opportunity[]
    paidWinsNoSEO: Opportunity[]
    bothWinning: Synergy[]
  }
  
  // Generate recommendations
  recommendActions(gaps: Gaps): PaidOrganicRecommendation[]
}
```

### Phase 4: Database Schema & Storage (Day 3-4)

#### Task 4.1: Database Connection Management
```typescript
// src/database/connection-pool.ts
export class DatabaseConnectionPool {
  private pool: Database[] = [];
  private readonly maxConnections = 10;
  private readonly connectionTimeout = 30000; // 30s
  
  async getConnection(): Promise<Database> {
    // Connection pooling for concurrent operations
  }
  
  async withTransaction<T>(
    operation: (db: Database) => Promise<T>
  ): Promise<T> {
    // Proper transaction isolation for concurrent writes
    const connection = await this.getConnection();
    await connection.run('BEGIN TRANSACTION');
    try {
      const result = await operation(connection);
      await connection.run('COMMIT');
      return result;
    } catch (error) {
      await connection.run('ROLLBACK');
      throw error;
    }
  }
}
```

#### Task 4.2: New Tables with Concurrency Controls
```sql
-- Search terms performance (partitioned by date for performance)
CREATE TABLE fact_search_terms (
  date DATE,
  engine TEXT DEFAULT 'google',
  campaign_id TEXT,
  ad_group_id TEXT,
  query TEXT,
  match_type TEXT,
  clicks INTEGER,
  impressions INTEGER,
  cost REAL,
  conversions REAL,
  conv_value REAL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (date, campaign_id, ad_group_id, query)
);

-- Add index for concurrent access patterns
CREATE INDEX idx_search_terms_date ON fact_search_terms(date);
CREATE INDEX idx_search_terms_performance ON fact_search_terms(date, cost DESC);

-- Quality Score tracking with version control
CREATE TABLE fact_qs (
  date DATE,
  campaign_id TEXT,
  ad_group_id TEXT,
  expected_ctr INTEGER,
  ad_relevance INTEGER,
  lp_experience INTEGER,
  quality_score INTEGER,
  version INTEGER DEFAULT 1,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (date, campaign_id, ad_group_id, version)
);

-- Event tracking (optional) with deduplication
CREATE TABLE fact_events (
  id TEXT PRIMARY KEY,
  date DATE,
  source TEXT,
  event_name TEXT,
  sessions INTEGER,
  count INTEGER,
  hash TEXT UNIQUE, -- Prevent duplicate events
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Negative keyword tracking with conflict resolution
CREATE TABLE proposed_negatives (
  id TEXT PRIMARY KEY,
  proposed_date DATE,
  keyword TEXT,
  match_type TEXT,
  placement_level TEXT,
  reason TEXT,
  waste_amount REAL,
  status TEXT DEFAULT 'proposed',
  applied_date DATE,
  conflict_resolution TEXT, -- Track how conflicts were resolved
  created_by TEXT DEFAULT 'v1.4',
  version INTEGER DEFAULT 1,
  UNIQUE(keyword, match_type, placement_level)
);
```

### Phase 5: CLI Commands & Orchestration (Day 4)

#### Task 5.1: Organized CLI Commands
```typescript
// src/cli-v14.ts - Enhanced with command organization
program
  .command('performance')
  .description('Performance analysis commands')
  .addCommand(
    new Command('ingest-ads')
      .description('Import Google Ads performance data')
      .option('--product <product>', 'Product name')
      .option('--from <source>', 'Data source: gaql|csv')
      .option('--file <path>', 'CSV file path if source=csv')
      .option('--batch-size <size>', 'Batch size for memory management', '1000')
      .action(ingestAdsCommand)
  )
  .addCommand(
    new Command('analyze-waste')
      .description('Analyze waste and propose negative keywords')
      .option('--product <product>', 'Product name')
      .option('--window <days>', 'Analysis window (default: 30d)')
      .option('--min-spend <amount>', 'Minimum wasted spend threshold')
      .option('--min-impr <count>', 'Minimum impressions threshold')
      .option('--memory-limit <mb>', 'Memory limit for processing', '512')
      .action(proposeNegativesCommand)
  )
  .addCommand(
    new Command('quality-score')
      .description('Analyze and triage Quality Score issues')
      .option('--product <product>', 'Product name')
      .option('--concurrent-checks <n>', 'Concurrent URL health checks', '5')
      .action(qsTriageCommand)
  )
  .addCommand(
    new Command('paid-organic-gaps')
      .description('Analyze paid/organic gaps')
      .option('--product <product>', 'Product name')
      .action(paidOrganicCommand)
  );

// Memory management utilities
class MemoryAwareProcessor {
  private readonly maxMemoryMB: number;
  
  constructor(maxMemoryMB = 512) {
    this.maxMemoryMB = maxMemoryMB;
  }
  
  async processInBatches<T, R>(
    items: T[],
    processor: (batch: T[]) => Promise<R[]>,
    batchSize = 1000
  ): Promise<R[]> {
    const results: R[] = [];
    
    for (let i = 0; i < items.length; i += batchSize) {
      const batch = items.slice(i, i + batchSize);
      const batchResults = await processor(batch);
      results.push(...batchResults);
      
      // Memory pressure check
      const memoryUsage = process.memoryUsage();
      if (memoryUsage.heapUsed / 1024 / 1024 > this.maxMemoryMB) {
        logger.warn('Memory pressure detected, forcing garbage collection');
        if (global.gc) global.gc();
        await new Promise(resolve => setImmediate(resolve));
      }
    }
    
    return results;
  }
}
```

### Phase 6: Output Writers (Day 4-5)

#### Task 6.1: Waste Report Writer
```typescript
// src/writers/waste-report-writer.ts
export class WasteReportWriter {
  generateReport(wastedTerms: WastedTerm[]): string {
    // Markdown report with:
    // - Executive summary (total waste, top offenders)
    // - Detailed n-gram analysis
    // - Proposed negative keywords by level
    // - Expected savings calculation
  }
}
```

#### Task 6.2: Negatives CSV Writer
```typescript
// src/writers/negatives-csv-writer.ts
export class NegativesCsvWriter {
  // Google Ads Editor format
  generateEditorCSV(negatives: NegativeKeyword[]): string
  
  // API format
  generateAPIJson(negatives: NegativeKeyword[]): object
}
```

## Testing Strategy

### Unit Tests
- N-gram extraction with various inputs
- Negative keyword placement logic
- QS issue categorization
- Gap analysis algorithms

### Integration Tests
- GAQL query execution
- CSV import validation
- Database persistence
- End-to-end workflow

### Acceptance Tests
1. **Negatives Import**: CSV imports cleanly in Google Ads Editor
2. **Waste Identification**: Report shows concrete n-grams with spend data
3. **QS Fixes**: At least 3 specific recommendations per low-QS ad group
4. **Idempotency**: No duplicate negatives re-proposed

## Risk Mitigation

### Technical Risks
1. **Low Volume Data**: Widen analysis window to 60-90 days
2. **Conversion Lag**: Implement "do-not-suggest-again" list with 30-day expiry
3. **API Quotas**: Aggressive caching, batch operations

### Business Risks
1. **Over-blocking**: Conservative thresholds, manual review required
2. **False Negatives**: Track applied negatives performance for 14 days

## Success Metrics

### Immediate (Day 1 after launch)
- Waste report identifies >$100 in wasted spend
- At least 20 negative keywords proposed
- QS triage covers 80% of ad groups

### Week 1
- 10% reduction in wasted spend
- Average QS improvement of 0.5 points
- 5 high-value paid/organic gaps identified

### Month 1
- 25% waste reduction
- 1-point QS improvement average
- CTR improvement of 15%

## Implementation Schedule

### Day 1: Data Ingestion
- [ ] Morning: Google Ads Performance connector (GAQL)
- [ ] Afternoon: CSV import fallback
- [ ] Evening: Analytics connector (GA4/Plausible)

### Day 2: Analysis Engine
- [ ] Morning: N-gram processor
- [ ] Afternoon: Negative placement rules
- [ ] Evening: Unit tests for analyzers

### Day 3: Quality & Gaps
- [ ] Morning: Quality Score analyzer
- [ ] Afternoon: Paid/Organic gap analyzer
- [ ] Evening: Database schema implementation

### Day 4: CLI & Integration
- [ ] Morning: CLI commands
- [ ] Afternoon: Orchestration logic
- [ ] Evening: Integration tests

### Day 5: Writers & Polish
- [ ] Morning: Report writers
- [ ] Afternoon: CSV/JSON exporters
- [ ] Evening: End-to-end testing, documentation

## Dependencies

### External APIs
- Google Ads API (search_term_view, ad_group_criterion)
- Google Analytics 4 API (optional)
- Plausible API (optional)

### Internal Systems
- Existing orchestrator framework
- Cache manager
- Database connection pool

## Documentation Requirements

1. **User Guide**: How to interpret waste reports and apply negatives
2. **API Reference**: New CLI commands and options
3. **Technical Docs**: N-gram algorithm, placement rules

## Rollout Plan

### Phase 1: Internal Testing
- Run on ConvertMyFile data
- Manual review of all proposals
- Track actual savings

### Phase 2: Soft Launch
- Enable for all products
- Daily monitoring of proposals
- A/B test negative application

### Phase 3: Full Automation
- Auto-apply low-risk negatives
- Slack alerts for high waste
- Weekly performance reports

## Future Enhancements

1. **Machine Learning**: Predict waste before it happens
2. **Competitor Analysis**: Identify competitor bidding patterns
3. **Budget Optimization**: Redistribute budget from waste to winners
4. **Custom Alerts**: Configurable thresholds and channels

---

**Estimated Effort**: 3-5 days
**Complexity**: Medium-High
**ROI**: High (immediate cost savings)
**Risk**: Low (read-only analysis, manual application)