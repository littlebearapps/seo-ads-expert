# v1.9 — Link Graph, Sitemaps & Indexation Control (incl. Bing)

## 0) Scope & outcomes

Objective: make sure search engines crawl, understand, and index what you ship. Eliminate orphans, fix coverage gaps, and formalize sitemaps/robots.

You’ll ship:
	•	A small crawler + link graph builder (internal only).
	•	Sitemap emitters (sectioned) + robots.txt audit.
	•	Indexation audit joining GSC coverage with your graph.
	•	Optional Bing: IndexNow pings + Webmaster verification helper.

⸻

## 1) Inputs you already have (reuse)
	•	Site URL list (from repo/hardcoded hubs + seo_pages.md).
	•	url_health.json (status/noindex/soft-404).
	•	(After 1.8) internal link suggestions + content map.
	•	GSC Coverage export (CSV or API read) for the root property.

New lightweight inputs
	•	Small internal crawler with polite limits (no external fetches off-domain).

⸻

## 2) Artifacts

plans/<product_or_site>/<date>/
  link_graph.json                  # nodes, edges, attributes
  internal_link_opportunities.csv  # source → anchor → target (ranked)
  sitemaps/
    core.xml
    use-cases.xml
    blog.xml
    images.xml                     # optional, for store badges/screens
  robots_audit.md                  # disallow checks, sitemap pointers
  indexation_gaps.json             # pages & reasons (GSC + heuristics)
  bing_webmaster.md                # verify + sitemap submit steps
  indexnow_ping.log                # optional, recent ping results
  
  Contracts

link_graph.json

{
  "nodes":[{"url":"/convertmyfile/webp-to-png","type":"use-case","status":200,"in":5,"out":8}],
  "edges":[{"src":"/convertmyfile/","dst":"/convertmyfile/webp-to-png","anchor":"WebP→PNG converter","rel":"contextual"}],
  "summary":{"orphans":0,"avg_out":7.2,"avg_in":4.8}
}

internal_link_opportunities.csv
| source_url | anchor_text | target_url | rationale | strength(1–3) | rel | section_hint |

indexation_gaps.json

{
  "summary":{"discovered_not_indexed":3,"crawled_not_indexed":2,"excluded_duplicates":1},
  "items":[
    {"url":"/palettekit/palette-generator","state":"Discovered - currently not indexed","fix":"add internal links; include in sitemap; ensure unique H1/meta"},
    {"url":"/notebridge/web-clipper","state":"Crawled - currently not indexed","fix":"increase content depth; avoid near-duplicate"}
  ]
}

3) CLI

seo-ads link-graph --site https://littlebearapps.com --max-depth 4 --budget 500
seo-ads sitemap-emit --sections core,use-cases,blog --pretty
seo-ads robots-audit --site https://littlebearapps.com
seo-ads indexation-audit --site https://littlebearapps.com --gsc path/to/coverage.csv
seo-ads indexnow --urls path/to/urls.txt --dry-run

## 4) DB (minimal)

CREATE TABLE fact_crawl (
  captured_at TEXT, url TEXT, status INT, content_hash TEXT,
  out_links INT, in_links INT, noindex INT, canonical TEXT,
  PRIMARY KEY (captured_at, url)
);
CREATE TABLE fact_indexation (
  date TEXT, url TEXT, gsc_state TEXT, PRIMARY KEY (date, url)
);


⸻

## 5) Algorithms & heuristics (plain English)

Crawler
	•	Start from known hubs & sitemap seeds, cap depth (≤4) and pages (≤500/site run).
	•	Fetch HTML, extract:
	•	links (a[href], rel), canonical, robots meta, H1/title, word count.
	•	Respect robots.txt; set concurrency 2–3; timeout 6–8s.

Orphans
	•	Nodes with in=0 and should-index (200, not noindex) → orphan.

Internal link opportunities
	•	For each use-case page, ensure:
	•	Inbound links ≥3 from distinct hubs/blogs.
	•	Contextual anchors include exact/partial/semantic mix.
	•	Rank opportunities by: target importance (use-case > product > blog), source authority (hubs > pillars > blog), and semantic fit.

Sitemaps
	•	Partition by section; include <lastmod> (from git or crawl date), set <priority> heuristically (use-case 0.8, product hubs 0.9, blog 0.5).
	•	Images: include store badges/screens if present.

Indexation join
	•	For each URL:
	•	Combine GSC state + crawler facts + url_health.
	•	Produce fix advice: add internal links, increase unique content, remove conflicting canonicals, avoid parameterized duplicates.

Robots audit
	•	Ensure Sitemap: lines present.
	•	Flag unintended disallows (e.g., blocking /extensions/).
	•	Warn if essential assets (JS/CSS) blocked.

Bing
	•	Emit bing_webmaster.md: site verify steps; submit sitemaps.
	•	Optional IndexNow ping for new/changed use-case URLs (respect per-day caps).

⸻

## 6) Guardrails
	•	Never suggest links to noindex or non-200 targets.
	•	Limit per-page new links (≤3) and vary anchors.
	•	Don’t add low-value pages to sitemaps (thin, 4xx, parameterized duplicates).

⸻

## 7) Testing
	•	Fixtures with mini-site (10–15 pages): validate orphan detection, link opportunities, and sitemap XML against golden snapshots.
	•	Import a real GSC CSV (scrubbed) and assert join logic (statuses + fixes).
	•	Robots tests: simulate blocked /extensions/ and confirm high-severity flag.

⸻

## 8) Acceptance gates
	•	Zero orphan use-case pages after applying suggested links.
	•	Sitemaps validate (XML + Search Console accept).
	•	indexation_gaps.json lists specific URLs with actionable fixes (not generic advice).
	•	Robots audit flags any sitemap omissions or accidental disallows.

⸻

## 9) Effort & risks
	•	Effort: 3–5 days.
	•	Risks: JS-rendered content; mitigate by server HTML snapshots or prebuilt text. Keep crawler polite; cap scope.